{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOcHFfb4fTKz",
        "outputId": "67f459be-230a-4f85-852d-36b331cf2302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.4)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install timm einops tensorboard tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "import timm\n",
        "from timm.models.layers import trunc_normal_\n",
        "from timm.models.vision_transformer import Block\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8sQ3gx_fYbv",
        "outputId": "0dc7d020-7d9d-42e1-fc43-3f1869719f47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "setup_seed(42)\n"
      ],
      "metadata": {
        "id": "37KHjBkbfadA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_indexes(size):\n",
        "    forward = np.arange(size)\n",
        "    np.random.shuffle(forward)\n",
        "    backward = np.argsort(forward)\n",
        "    return forward, backward\n",
        "\n",
        "def take_indexes(x, indexes):\n",
        "    return torch.gather(x, 0, repeat(indexes, 't b -> t b c', c=x.shape[-1]))\n"
      ],
      "metadata": {
        "id": "J3YiGelofdfx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchShuffle(nn.Module):\n",
        "    def __init__(self, ratio):\n",
        "        super().__init__()\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, patches):\n",
        "        T, B, C = patches.shape\n",
        "        remain = int(T * (1 - self.ratio))\n",
        "\n",
        "        indexes = [random_indexes(T) for _ in range(B)]\n",
        "        forward = torch.tensor(np.stack([i[0] for i in indexes], axis=-1)).to(patches.device)\n",
        "        backward = torch.tensor(np.stack([i[1] for i in indexes], axis=-1)).to(patches.device)\n",
        "\n",
        "        patches = take_indexes(patches, forward)\n",
        "        patches = patches[:remain]\n",
        "\n",
        "        return patches, forward, backward\n"
      ],
      "metadata": {
        "id": "d4MLiEjtffJv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAE_Encoder(nn.Module):\n",
        "    def __init__(self, image_size=32, patch_size=2, emb_dim=192,\n",
        "                 num_layer=12, num_head=3, mask_ratio=0.75):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.zeros((image_size // patch_size) ** 2, 1, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.patchify = nn.Conv2d(3, emb_dim, patch_size, patch_size)\n",
        "        self.shuffle = PatchShuffle(mask_ratio)\n",
        "\n",
        "        self.transformer = nn.Sequential(\n",
        "            *[Block(emb_dim, num_head) for _ in range(num_layer)]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        trunc_normal_(self.pos_embedding, std=.02)\n",
        "\n",
        "    def forward(self, img):\n",
        "        patches = self.patchify(img)\n",
        "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
        "        patches = patches + self.pos_embedding\n",
        "\n",
        "        patches, _, backward = self.shuffle(patches)\n",
        "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
        "\n",
        "        patches = rearrange(patches, 't b c -> b t c')\n",
        "        features = self.norm(self.transformer(patches))\n",
        "        features = rearrange(features, 'b t c -> t b c')\n",
        "\n",
        "        return features, backward\n"
      ],
      "metadata": {
        "id": "euHZ6CWtfhV7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAE_Decoder(nn.Module):\n",
        "    def __init__(self, image_size=32, patch_size=2, emb_dim=192,\n",
        "                 num_layer=4, num_head=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.transformer = nn.Sequential(\n",
        "            *[Block(emb_dim, num_head) for _ in range(num_layer)]\n",
        "        )\n",
        "\n",
        "        self.head = nn.Linear(emb_dim, 3 * patch_size ** 2)\n",
        "        self.patch2img = Rearrange(\n",
        "            '(h w) b (c p1 p2) -> b c (h p1) (w p2)',\n",
        "            p1=patch_size, p2=patch_size, h=image_size//patch_size\n",
        "        )\n",
        "\n",
        "        trunc_normal_(self.mask_token, std=.02)\n",
        "        trunc_normal_(self.pos_embedding, std=.02)\n",
        "\n",
        "    def forward(self, features, backward):\n",
        "        T = features.shape[0]\n",
        "\n",
        "        backward = torch.cat([torch.zeros(1, backward.shape[1]).to(backward), backward + 1])\n",
        "        features = torch.cat([\n",
        "            features,\n",
        "            self.mask_token.expand(backward.shape[0] - T, features.shape[1], -1)\n",
        "        ])\n",
        "\n",
        "        features = take_indexes(features, backward)\n",
        "        features = features + self.pos_embedding\n",
        "\n",
        "        features = rearrange(features, 't b c -> b t c')\n",
        "        features = self.transformer(features)\n",
        "        features = rearrange(features, 'b t c -> t b c')[1:]\n",
        "\n",
        "        patches = self.head(features)\n",
        "        mask = torch.zeros_like(patches)\n",
        "        mask[T-1:] = 1\n",
        "        mask = take_indexes(mask, backward[1:] - 1)\n",
        "\n",
        "        return self.patch2img(patches), self.patch2img(mask)\n"
      ],
      "metadata": {
        "id": "wx_V3FhWfkVY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAE_ViT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = MAE_Encoder()\n",
        "        self.decoder = MAE_Decoder()\n",
        "\n",
        "    def forward(self, img):\n",
        "        features, backward = self.encoder(img)\n",
        "        return self.decoder(features, backward)\n"
      ],
      "metadata": {
        "id": "5FY_UrE3fn2I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.CIFAR10(\n",
        "    root='data', train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gywAEBf1fp_B",
        "outputId": "5e21ccc0-10b1-4612-8177-89bdb226226b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:19<00:00, 8.68MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = MAE_ViT().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-4)\n",
        "writer = SummaryWriter('logs/mae')\n",
        "\n",
        "for epoch in range(10):  # increase to 2000 later\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for img, _ in tqdm(train_loader):\n",
        "        img = img.to(device)\n",
        "        pred, mask = model(img)\n",
        "        loss = ((pred - img) ** 2 * mask).mean() / 0.75\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    avg = sum(losses)/len(losses)\n",
        "    writer.add_scalar('mae_loss', avg, epoch)\n",
        "    print(f\"Epoch {epoch} | Loss {avg:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMLgbJGoftyA",
        "outputId": "91dffdd0-103a-42d8-97eb-68d84db203f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:24<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss 0.1830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:25<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss 0.1521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:23<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Loss 0.1385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:24<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss 0.1254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:24<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Loss 0.1150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:23<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Loss 0.0951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:24<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Loss 0.0789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:24<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Loss 0.0726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:24<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Loss 0.0686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 196/196 [02:24<00:00,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Loss 0.0655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"vit_t_mae.pt\")"
      ],
      "metadata": {
        "id": "2XErSz2nfyRV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT_Classifier(nn.Module):\n",
        "    def __init__(self, encoder, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.cls_token = encoder.cls_token\n",
        "        self.pos_embedding = encoder.pos_embedding\n",
        "        self.patchify = encoder.patchify\n",
        "        self.transformer = encoder.transformer\n",
        "        self.norm = encoder.norm\n",
        "        self.head = nn.Linear(192, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        patches = self.patchify(img)\n",
        "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
        "        patches = patches + self.pos_embedding\n",
        "\n",
        "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches])\n",
        "        patches = rearrange(patches, 't b c -> b t c')\n",
        "\n",
        "        features = self.norm(self.transformer(patches))\n",
        "        return self.head(features[:, 0])"
      ],
      "metadata": {
        "id": "lFQFFVlJf2xk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.CIFAR10(\n",
        "    root='data', train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "val_ds = torchvision.datasets.CIFAR10(\n",
        "    root='data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=128, shuffle=False, num_workers=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "gONzegSSf3gX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# OPTION 1: from pretrained MAE\n",
        "mae_model = torch.load(\n",
        "    \"vit_t_mae.pt\",\n",
        "    map_location='cpu',\n",
        "    weights_only=False\n",
        ")\n",
        "\n",
        "encoder = mae_model.encoder\n",
        "\n",
        "# OPTION 2: from scratch (comment above, uncomment below if needed)\n",
        "# encoder = MAE_Encoder()\n"
      ],
      "metadata": {
        "id": "uAFqMlQIhVKN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = ViT_Classifier(encoder, num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "9gBK0tclhZO7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    return (logits.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    classifier.parameters(),\n",
        "    lr=1e-3 * 128 / 256,   # same LR scaling as repo\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=0.05\n",
        ")\n",
        "\n",
        "total_epochs = 20\n",
        "warmup_epochs = 5\n",
        "\n",
        "lr_fn = lambda epoch: min(\n",
        "    (epoch + 1) / (warmup_epochs + 1e-8),\n",
        "    0.5 * (math.cos(epoch / total_epochs * math.pi) + 1)\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n"
      ],
      "metadata": {
        "id": "CiCX-UvUhZu2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = torchvision.datasets.CIFAR10(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "YCWdb1zwoDTI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    # ---- TRAIN ----\n",
        "    classifier.train()\n",
        "    train_losses, train_accs = [], []\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        logits = classifier(imgs)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        acc = accuracy(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        train_accs.append(acc.item())\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "    avg_train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "    # ---- VALIDATION ----\n",
        "    classifier.eval()\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            logits = classifier(imgs)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            acc = accuracy(logits, labels)\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            val_accs.append(acc.item())\n",
        "\n",
        "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "    avg_val_acc = sum(val_accs) / len(val_accs)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch} | \"\n",
        "        f\"Train Acc: {avg_train_acc:.4f} | \"\n",
        "        f\"Val Acc: {avg_val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_acc > best_val_acc:\n",
        "        best_val_acc = avg_val_acc\n",
        "        torch.save(classifier, \"vit_classifier_best.pt\")\n",
        "        print(\"✅ Saved best model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "mP9XYLszhctK",
        "outputId": "8e95254a-cfbf-48bf-febc-fd12be4dbbd8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 2/196 [00:03<06:07,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3337676517.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "classifier = torch.load(\n",
        "    \"vit_classifier_best.pt\",\n",
        "    map_location=device,\n",
        "    weights_only=False\n",
        ")\n",
        "\n",
        "classifier.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB7Tnjshherz",
        "outputId": "a2b43c95-ccaf-4d52-c0ab-84ec51910230"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT_Classifier(\n",
              "  (patchify): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (transformer): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "TzTIsxl6rDDa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Take one image from validation set\n",
        "idx = 5\n",
        "img, label = val_ds[idx]\n",
        "\n",
        "# Show image\n",
        "plt.imshow(((img + 1) / 2).permute(1, 2, 0))\n",
        "plt.title(f\"True label: {classes[label]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "29yBfV1JrDWN",
        "outputId": "832b2c5d-ca6e-42ef-cda3-f56d7b1e0680"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    input_img = img.unsqueeze(0).to(device)  # add batch dimension\n",
        "    logits = classifier(input_img)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "pred_class = probs.argmax(dim=1).item()\n",
        "confidence = probs.max().item()\n",
        "\n",
        "print(f\"Predicted label: {classes[pred_class]}\")\n",
        "print(f\"Confidence: {confidence:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzTt0vNLrFb_",
        "outputId": "915600ea-7267-4c8a-eb64-c096afb7b7e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: frog\n",
            "Confidence: 0.7180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "VqVZXJsdrGwM",
        "outputId": "40f0eb2e-6dcb-44f9-cb29-a2205c9a6865"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e5e36eef-ff5c-4c72-842d-b1110f64537a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e5e36eef-ff5c-4c72-842d-b1110f64537a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kran.jpeg to kran.jpeg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "id_wjrbCticb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "img_path = list(uploaded.keys())[0]\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0.5, 0.5)\n",
        "])\n",
        "\n",
        "img = transform(img)\n",
        "\n",
        "plt.imshow(((img + 1) / 2).permute(1, 2, 0))\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "DjASq619rWDD",
        "outputId": "e5a8db46-8c87-4ba2-d8e7-25fab8e9fd51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits = classifier(img.unsqueeze(0).to(device))\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "pred = probs.argmax(dim=1).item()\n",
        "print(\"Predicted:\", classes[pred])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsoKrO4lrX9e",
        "outputId": "5886fa46-cc5f-4ac5-bdab-3be2dbb4c39c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: airplane\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLsuW19HtTIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}